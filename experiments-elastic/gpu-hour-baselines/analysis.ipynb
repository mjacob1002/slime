{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3.10 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all subfolders and extract GPU counts and training times\n",
    "base_dir = '/workspace/slime/experiments-elastic/gpu-hour-baselines/'\n",
    "\n",
    "# Find all log files\n",
    "log_files = glob.glob(os.path.join(base_dir, '*/*_log.txt'))\n",
    "\n",
    "data = []\n",
    "\n",
    "for log_file in log_files:\n",
    "    # Extract folder name to determine GPU configuration\n",
    "    folder_name = os.path.basename(os.path.dirname(log_file))\n",
    "    \n",
    "    # Parse inference and training GPU counts from folder name (e.g., '2i_1t' or '1i_1t_info')\n",
    "    match = re.match(r'(\\d+)i_(\\d+)t', folder_name)\n",
    "    if match:\n",
    "        inference_gpus = int(match.group(1))\n",
    "        training_gpus = int(match.group(2))\n",
    "        total_gpus = inference_gpus + training_gpus\n",
    "        \n",
    "        # Read the log file and extract training time\n",
    "        with open(log_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            time_match = re.search(r'Total training time: ([\\d.]+)', content)\n",
    "            if time_match:\n",
    "                training_time = float(time_match.group(1))\n",
    "                data.append({\n",
    "                    'folder': folder_name,\n",
    "                    'inference_gpus': inference_gpus,\n",
    "                    'training_gpus': training_gpus,\n",
    "                    'total_gpus': total_gpus,\n",
    "                    'training_time': training_time\n",
    "                })\n",
    "                print(f\"{folder_name}: {total_gpus} GPUs, {training_time:.2f}s\")\n",
    "\n",
    "# Sort by total GPUs\n",
    "data.sort(key=lambda x: x['total_gpus'])\n",
    "\n",
    "# Extract arrays for plotting\n",
    "total_gpus = [d['total_gpus'] for d in data]\n",
    "training_times = [d['training_time'] for d in data]\n",
    "gpu_hours = [d['training_time'] * d['total_gpus'] / 3600 for d in data]  # Convert to hours\n",
    "\n",
    "print(f\"\\nParsed {len(data)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_total_elastic_time(global_batch_size: int, total_gpus_used: int, number_of_dedicated_inference_gpus: float, number_of_elastic_gpus: int, gpu_inference_throughput: float, gpu_training_throughput: float, training_to_inference_cost: float, inference_to_training_cost: float):\n",
    "    dedicated_inference_throughput = number_of_dedicated_inference_gpus * gpu_inference_throughput\n",
    "    total_inference_throughput = total_gpus_used * gpu_inference_throughput\n",
    "    # the time for the first rollout is the throughput when every engine does inference\n",
    "    time = 0\n",
    "    time += (global_batch_size / total_inference_throughput)\n",
    "    num_rollouts = 5\n",
    "    # this is a loop that basically does training stuff\n",
    "    for i in range(num_rollouts):\n",
    "        time += inference_to_training_cost\n",
    "        time_training = global_batch_size / (gpu_training_throughput * number_of_elastic_gpus)\n",
    "        time += time_training\n",
    "\n",
    "        if i == num_rollouts - 1:\n",
    "            break  # Last training done, no need to switch back\n",
    "\n",
    "        time += training_to_inference_cost\n",
    "        total_time_used = inference_to_training_cost + time_training + training_to_inference_cost\n",
    "        remaining_samples_in_async_batch = global_batch_size - dedicated_inference_throughput * total_time_used\n",
    "        if remaining_samples_in_async_batch > 0:\n",
    "            extra_time_to_complete_next_batch = remaining_samples_in_async_batch / total_inference_throughput\n",
    "            time += extra_time_to_complete_next_batch\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sync_total_time(global_batch_size, total_gpus_used, gpu_inference_throughput, gpu_training_throughput, num_rollouts: int = 5):\n",
    "    total_inference_throughput, total_training_throughput = total_gpus_used * gpu_inference_throughput, total_gpus * gpu_training_throughput\n",
    "    inference_time, training_time = global_batch_size / total_inference_throughput, global_batch_size / total_training_throughput\n",
    "    return num_rollouts * (inference_time + training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_step_overlap_total_time(global_batch_size, total_gpus_used, num_inference_gpus, num_training_gpus, gpu_inference_throughput, gpu_training_throughput, num_rollouts: int = 5):\n",
    "    total_inference_time = global_batch_size / (num_inference_gpus * gpu_inference_throughput)\n",
    "    total_training_time = global_batch_size / (num_training_gpus * gpu_training_throughput)\n",
    "    total_time = total_inference_time + total_training_time + (num_rollouts - 1) * max(total_inference_time, total_training_time)\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental: calculating the elastic's theoretical best\n",
    "total_gpus_elastic = [i for i in range(2, 8)]\n",
    "elastic_theoretical_time = []\n",
    "# These constants are measured\n",
    "GPU_INFERENCE_THROUGHPUT = 1\n",
    "GPU_TRAINING_THROUGHPUT = 2.5\n",
    "TRAINING_TO_INFERENCE = 1.8\n",
    "INFERENCE_TO_TRAINING = 3.01\n",
    "# test the value of a setting\n",
    "time = simulate_total_time(256, 2, 1, 1, GPU_INFERENCE_THROUGHPUT, GPU_TRAINING_THROUGHPUT, TRAINING_TO_INFERENCE, INFERENCE_TO_TRAINING)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with dual y-axis\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot 1: Total training time (left y-axis)\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Total GPUs', fontsize=12)\n",
    "ax1.set_ylabel('Training Time (seconds)', color=color1, fontsize=12)\n",
    "line1 = ax1.plot(total_gpus, training_times, 'o-', color=color1, linewidth=2, markersize=8, label='Training Time')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.set_xticks(total_gpus)\n",
    "\n",
    "# Plot 2: GPU-hours (right y-axis)\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'tab:orange'\n",
    "ax2.set_ylabel('GPU-Hours', color=color2, fontsize=12)\n",
    "line2 = ax2.plot(total_gpus, gpu_hours, 's--', color=color2, linewidth=2, markersize=8, label='GPU-Hours')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.title('Elastic Training: Time and GPU-Hours vs Total GPUs', fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"{'GPUs':<6} {'Time (s)':<12} {'GPU-Hours':<12}\")\n",
    "print(\"-\" * 30)\n",
    "for d in data:\n",
    "    gpu_hr = d['training_time'] * d['total_gpus'] / 3600\n",
    "    print(f\"{d['total_gpus']:<6} {d['training_time']:<12.2f} {gpu_hr:<12.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
